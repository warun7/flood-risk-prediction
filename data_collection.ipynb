{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting earthengine-api\n",
      "  Using cached earthengine_api-1.5.5-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting geemap\n",
      "  Using cached geemap-0.35.3-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting google-cloud-storage (from earthengine-api)\n",
      "  Using cached google_cloud_storage-3.1.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting google-api-python-client>=1.12.1 (from earthengine-api)\n",
      "  Using cached google_api_python_client-2.163.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=1.4.1 (from earthengine-api)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from earthengine-api)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from earthengine-api) (0.22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from earthengine-api) (2.32.3)\n",
      "Collecting bqplot (from geemap)\n",
      "  Using cached bqplot-0.12.44-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: colour in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (0.1.5)\n",
      "Collecting eerepr>=0.1.0 (from geemap)\n",
      "  Using cached eerepr-0.1.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting folium>=0.17.0 (from geemap)\n",
      "  Using cached folium-0.19.5-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting geocoder (from geemap)\n",
      "  Using cached geocoder-1.38.1-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting ipyevents (from geemap)\n",
      "  Using cached ipyevents-2.0.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting ipyfilechooser>=0.6.0 (from geemap)\n",
      "  Using cached ipyfilechooser-0.6.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting ipyleaflet>=0.19.2 (from geemap)\n",
      "  Using cached ipyleaflet-0.19.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting ipytree (from geemap)\n",
      "  Using cached ipytree-0.2.2-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (2.2.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (6.0.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (1.9.0)\n",
      "Requirement already satisfied: pyshp>=2.3.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (2.3.1)\n",
      "Requirement already satisfied: python-box in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (7.3.2)\n",
      "Requirement already satisfied: scooby in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geemap) (0.10.0)\n",
      "Collecting branca>=0.6.0 (from folium>=0.17.0->geemap)\n",
      "  Using cached branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium>=0.17.0->geemap) (3.1.4)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from folium>=0.17.0->geemap) (2025.1.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client>=1.12.1->earthengine-api)\n",
      "  Using cached google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client>=1.12.1->earthengine-api) (4.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.4.1->earthengine-api) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.4.1->earthengine-api) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.4.1->earthengine-api) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1dev,>=0.9.2->earthengine-api) (3.1.4)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipyfilechooser>=0.6.0->geemap) (8.1.5)\n",
      "Requirement already satisfied: jupyter-leaflet<0.20,>=0.19 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipyleaflet>=0.19.2->geemap) (0.19.2)\n",
      "Requirement already satisfied: traittypes<3,>=0.2.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipyleaflet>=0.19.2->geemap) (0.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.0 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from bqplot->geemap) (5.14.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->geemap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->geemap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->geemap) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geocoder->geemap) (8.1.8)\n",
      "Requirement already satisfied: future in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geocoder->geemap) (1.0.0)\n",
      "Requirement already satisfied: ratelim in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geocoder->geemap) (0.1.6)\n",
      "Requirement already satisfied: six in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from geocoder->geemap) (1.16.0)\n",
      "Collecting google-cloud-core<3.0dev,>=2.4.2 (from google-cloud-storage->earthengine-api)\n",
      "  Using cached google_cloud_core-2.4.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage->earthengine-api)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-cloud-storage->earthengine-api) (1.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->earthengine-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->earthengine-api) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->earthengine-api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->earthengine-api) (2024.8.30)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->geemap) (10.4.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from plotly->geemap) (1.29.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.69.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.26.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets->ipyfilechooser>=0.6.0->geemap) (8.27.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->ipyfilechooser>=0.6.0->geemap) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->ipyfilechooser>=0.6.0->geemap) (3.0.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2>=2.9->folium>=0.17.0->geemap) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\advai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from click->geocoder->geemap) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ratelim->geocoder->geemap) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\advai\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser>=0.6.0->geemap) (0.2.3)\n",
      "Using cached earthengine_api-1.5.5-py3-none-any.whl (459 kB)\n",
      "Using cached geemap-0.35.3-py2.py3-none-any.whl (2.3 MB)\n",
      "Using cached eerepr-0.1.1-py3-none-any.whl (9.6 kB)\n",
      "Using cached folium-0.19.5-py2.py3-none-any.whl (110 kB)\n",
      "Using cached google_api_python_client-2.163.0-py2.py3-none-any.whl (13.1 MB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
      "Using cached ipyleaflet-0.19.2-py3-none-any.whl (31 kB)\n",
      "Using cached bqplot-0.12.44-py2.py3-none-any.whl (1.2 MB)\n",
      "Using cached geocoder-1.38.1-py2.py3-none-any.whl (98 kB)\n",
      "Using cached google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
      "Using cached ipyevents-2.0.2-py3-none-any.whl (101 kB)\n",
      "Using cached ipytree-0.2.2-py2.py3-none-any.whl (1.3 MB)\n",
      "Using cached branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Using cached google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_cloud_core-2.4.2-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Installing collected packages: google-resumable-media, google-auth, geocoder, branca, google-auth-httplib2, google-api-core, folium, google-cloud-core, google-api-python-client, ipytree, ipyleaflet, ipyfilechooser, ipyevents, google-cloud-storage, bqplot, earthengine-api, eerepr, geemap\n",
      "Successfully installed bqplot-0.12.44 branca-0.8.1 earthengine-api-1.5.5 eerepr-0.1.1 folium-0.19.5 geemap-0.35.3 geocoder-1.38.1 google-api-core-2.24.1 google-api-python-client-2.163.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-cloud-core-2.4.2 google-cloud-storage-3.1.0 google-resumable-media-2.7.2 ipyevents-2.0.2 ipyfilechooser-0.6.0 ipyleaflet-0.19.2 ipytree-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install earthengine-api geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Mwop390PTBznH9NVuXPiseB9BHOgonzoSzjWum33Rm8&tc=vwEggbZ-R5stcTMKy4yiKP8gSr0WrSe-qrv7rYnSCY8&cc=qPSw_s3FeO-FbHzAfGqZaIaR5GOl2O6E0-cuKuCv43Q>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Mwop390PTBznH9NVuXPiseB9BHOgonzoSzjWum33Rm8&tc=vwEggbZ-R5stcTMKy4yiKP8gSr0WrSe-qrv7rYnSCY8&cc=qPSw_s3FeO-FbHzAfGqZaIaR5GOl2O6E0-cuKuCv43Q</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Authenticate and initialize\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images for location (26.2074, 82.6165) from 2015-07-15 to 2015-08-19\n",
      "No suitable images found for location (26.2074, 82.6165). Skipping...\n",
      "Found 0 images for location (26.8946, 93.751) from 2015-08-13 to 2015-11-09\n",
      "No suitable images found for location (26.8946, 93.751). Skipping...\n",
      "Invalid date range at index 2: 2015-10-11 00:00:00 to 2015-04-12 00:00:00. Swapping dates.\n",
      "Found 7 images for location (11.8278, 78.8554) from 2015-04-12 to 2015-10-11\n",
      "Exporting 2 to Google Drive...\n",
      "Invalid date range at index 3: 2016-04-20 00:00:00 to 2016-01-05 00:00:00. Swapping dates.\n",
      "Found 22 images for location (27.464, 95.6068) from 2016-01-05 to 2016-04-20\n",
      "Exporting 3 to Google Drive...\n",
      "Found 2 images for location (27.068, 93.949) from 2016-06-29 to 2016-08-26\n",
      "Exporting 4 to Google Drive...\n",
      "Invalid date range at index 5: 2016-07-07 00:00:00 to 2016-03-08 00:00:00. Swapping dates.\n",
      "Found 26 images for location (22.8107, 80.8349) from 2016-03-08 to 2016-07-07\n",
      "Exporting 5 to Google Drive...\n",
      "Found 14 images for location (21.7477, 73.4119) from 2016-01-08 to 2016-03-08\n",
      "Exporting 6 to Google Drive...\n",
      "Found 4 images for location (25.6099, 85.1898) from 2016-07-15 to 2016-09-22\n",
      "Exporting 7 to Google Drive...\n",
      "Found 0 images for location (17.6536, 80.3401) from 2016-09-21 to 2016-10-16\n",
      "No suitable images found for location (17.6536, 80.3401). Skipping...\n",
      "Found 4 images for location (27.6131, 94.9387) from 2017-02-06 to 2017-03-07\n",
      "Exporting 9 to Google Drive...\n",
      "Found 0 images for location (21.898, 70.9375) from 2017-07-15 to 2017-07-20\n",
      "No suitable images found for location (21.898, 70.9375). Skipping...\n",
      "Found 2 images for location (22.9061, 88.0848) from 2017-07-27 to 2017-10-08\n",
      "Exporting 11 to Google Drive...\n",
      "Invalid date range at index 12: 2017-10-08 00:00:00 to 2017-08-26 00:00:00. Swapping dates.\n",
      "Found 1 images for location (28.1657, 79.076) from 2017-08-26 to 2017-10-08\n",
      "Exporting 12 to Google Drive...\n",
      "Invalid date range at index 13: 2017-10-30 00:00:00 to 2017-08-11 00:00:00. Swapping dates.\n",
      "Found 9 images for location (10.489, 79.0039) from 2017-08-11 to 2017-10-30\n",
      "Exporting 13 to Google Drive...\n",
      "Found 0 images for location (14.3584, 75.4408) from 2018-05-29 to 2018-05-30\n",
      "No suitable images found for location (14.3584, 75.4408). Skipping...\n",
      "Invalid date range at index 15: 2018-12-06 00:00:00 to 2018-06-20 00:00:00. Swapping dates.\n",
      "Found 85 images for location (26.8948, 93.3551) from 2018-06-20 to 2018-12-06\n",
      "Exporting 15 to Google Drive...\n",
      "Found 47 images for location (33.3752, 75.1439) from 2018-06-25 to 2018-11-07\n",
      "Exporting 16 to Google Drive...\n",
      "Invalid date range at index 17: 2018-06-22 00:00:00 to 2018-02-07 00:00:00. Swapping dates.\n",
      "Found 138 images for location (20.9795, 73.783) from 2018-02-07 to 2018-06-22\n",
      "Exporting 17 to Google Drive...\n",
      "Found 34 images for location (19.5656, 74.5501) from 2018-06-07 to 2018-11-07\n",
      "Exporting 18 to Google Drive...\n",
      "Invalid date range at index 19: 2018-09-07 00:00:00 to 2018-07-19 00:00:00. Swapping dates.\n",
      "Found 2 images for location (12.5238, 75.6883) from 2018-07-19 to 2018-09-07\n",
      "Exporting 19 to Google Drive...\n",
      "Found 7 images for location (9.39943, 76.7275) from 2018-07-08 to 2018-10-08\n",
      "Exporting 20 to Google Drive...\n",
      "Found 19 images for location (27.2914, 94.1222) from 2018-02-08 to 2018-10-08\n",
      "Exporting 21 to Google Drive...\n",
      "Found 166 images for location (25.7354, 82.4309) from 2018-01-09 to 2018-07-09\n",
      "Exporting 22 to Google Drive...\n",
      "Invalid date range at index 23: 2018-08-15 00:00:00 to 2018-07-09 00:00:00. Swapping dates.\n",
      "Found 8 images for location (26.0513, 94.7655) from 2018-07-09 to 2018-08-15\n",
      "Exporting 23 to Google Drive...\n",
      "Invalid date range at index 24: 2018-09-23 00:00:00 to 2018-02-10 00:00:00. Swapping dates.\n",
      "Found 118 images for location (31.3414, 76.6285) from 2018-02-10 to 2018-09-23\n",
      "Exporting 24 to Google Drive...\n",
      "Found 28 images for location (17.9, 83.3588) from 2018-11-10 to 2018-12-10\n",
      "Exporting 25 to Google Drive...\n",
      "Found 56 images for location (21.5427, 87.4662) from 2019-03-05 to 2019-07-05\n",
      "Exporting 26 to Google Drive...\n",
      "Found 0 images for location (23.4991, 91.5736) from 2019-05-24 to 2019-05-27\n",
      "No suitable images found for location (23.4991, 91.5736). Skipping...\n",
      "Found 0 images for location (27.3531, 94.3944) from 2019-06-27 to 2019-07-13\n",
      "No suitable images found for location (27.3531, 94.3944). Skipping...\n",
      "Found 86 images for location (20.2969, 74.748) from 2019-01-07 to 2019-05-07\n",
      "Exporting 29 to Google Drive...\n",
      "Found 0 images for location (11.4575, 76.0842) from 2019-06-08 to 2019-09-08\n",
      "No suitable images found for location (11.4575, 76.0842). Skipping...\n",
      "Found 32 images for location (19.1145, 83.4825) from 2019-05-08 to 2019-09-08\n",
      "Exporting 31 to Google Drive...\n",
      "Found 0 images for location (25.3928, 74.3521) from 2019-09-13 to 2019-09-22\n",
      "No suitable images found for location (25.3928, 74.3521). Skipping...\n",
      "Found 0 images for location (24.1511, 77.3956) from 2019-09-15 to 2019-09-22\n",
      "No suitable images found for location (24.1511, 77.3956). Skipping...\n",
      "Invalid date range at index 34: 2019-10-25 00:00:00 to 2019-06-11 00:00:00. Swapping dates.\n",
      "Found 10 images for location (27.0678, 94.2459) from 2019-06-11 to 2019-10-25\n",
      "Exporting 34 to Google Drive...\n",
      "Invalid date range at index 35: 2019-11-30 00:00:00 to 2019-06-12 00:00:00. Swapping dates.\n",
      "Found 41 images for location (10.8488, 78.8554) from 2019-06-12 to 2019-11-30\n",
      "Exporting 35 to Google Drive...\n",
      "Found 0 images for location (26.2074, 82.62) from 2015-07-15 to 2015-08-19\n",
      "No suitable images found for location (26.2074, 82.62). Skipping...\n",
      "Found 0 images for location (26.8946, 93.75) from 2015-08-13 to 2015-08-19\n",
      "No suitable images found for location (26.8946, 93.75). Skipping...\n",
      "Invalid coordinates: (118.278, 788.55) at index 38. Skipping...\n",
      "Found 0 images for location (27.464, 95.607) from 2016-04-22 to 2016-04-25\n",
      "No suitable images found for location (27.464, 95.607). Skipping...\n",
      "Found 0 images for location (27.068, 93.95) from 2016-06-25 to 2016-07-01\n",
      "No suitable images found for location (27.068, 93.95). Skipping...\n",
      "Found 0 images for location (17.654, 80.34) from 2016-09-21 to 2016-09-29\n",
      "No suitable images found for location (17.654, 80.34). Skipping...\n",
      "Found 1 images for location (10.489, 79.004) from 2017-10-30 to 2017-11-08\n",
      "Exporting 42 to Google Drive...\n",
      "All export tasks initiated! Check Google Drive for results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ee\n",
    "import datetime\n",
    "\n",
    "# Authenticate & Initialize Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Load historical flood data\n",
    "df_historical = pd.read_csv('./datasets/floods_inventory/info.csv')\n",
    "\n",
    "# Filter data to include only floods after 2015\n",
    "# df_historical['Start Date'] = pd.to_datetime(df_historical['Start Date'], errors='coerce')\n",
    "# df_historical['End Date'] = pd.to_datetime(df_historical['End Date'], errors='coerce')\n",
    "# df_historical = df_historical[df_historical['Start Date'].dt.year >= 2015]\n",
    "df_filter = df_historical[df_historical['Start Date'].str.contains('2015|2016|2017|2018|2019|2020|2021|2022|2023')]\n",
    "# Define buffer size (e.g., 50 km)\n",
    "buffer_size = 50000  # 50 km\n",
    "\n",
    "def mask_s2_clouds(image):\n",
    "    qa = image.select('QA60')\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    return image.updateMask(mask).divide(10000)\n",
    "\n",
    "def get_s2_image(lat, lon, date, buffer_size_m=50000):\n",
    "    # Create point and buffer it\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    region = point.buffer(buffer_size_m).bounds()\n",
    "\n",
    "    # Define date range\n",
    "    start_date = ee.Date(date)\n",
    "    end_date = start_date.advance(1, 'day')  # one day window\n",
    "\n",
    "    # Load and filter collection\n",
    "    collection = (\n",
    "        ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filterBounds(region)\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "        .map(mask_s2_clouds)\n",
    "    )\n",
    "\n",
    "    # Get the first image (or median if needed)\n",
    "    image = collection.first().clip(region)\n",
    "    return image.select(['B2', 'B3', 'B4', 'B8', 'B11'])  # Blue, Green, Red, NIR, SWIR\n",
    "\n",
    "\n",
    "# # Cloud Mask Function (QA60-based)\n",
    "# def mask_s2_clouds(image):\n",
    "#     qa = image.select(\"QA60\")  # Sentinel-2 cloud mask band\n",
    "\n",
    "#     # Bits 10 and 11 are clouds and cirrus, respectively\n",
    "#     cloud_bit_mask = 1 << 10\n",
    "#     cirrus_bit_mask = 1 << 11\n",
    "\n",
    "#     # Both flags should be zero for clear pixels\n",
    "#     mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "#         qa.bitwiseAnd(cirrus_bit_mask).eq(0)\n",
    "#     )\n",
    "\n",
    "#     # Apply mask and scale the image\n",
    "#     return image.updateMask(mask).divide(10000)\n",
    "\n",
    "# # Loop through each row and extract imagery\n",
    "# for index, row in df_historical.iterrows():\n",
    "#     try:\n",
    "#         lat, lon = row['Latitude'], row['Longitude']\n",
    "        \n",
    "#         # Validate coordinates\n",
    "#         if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "#             print(f\"Invalid coordinates: ({lat}, {lon}) at index {index}. Skipping...\")\n",
    "#             continue\n",
    "            \n",
    "#         start_date, end_date = row['Start Date'], row['End Date']\n",
    "\n",
    "#         # Ensure valid date range\n",
    "#         if start_date >= end_date:\n",
    "#             print(f\"Invalid date range at index {index}: {start_date} to {end_date}. Swapping dates.\")\n",
    "#             start_date, end_date = end_date, start_date\n",
    "\n",
    "#         # Ensure at least one day difference\n",
    "#         if (end_date - start_date).days < 1:\n",
    "#             print(f\"Date range too short at index {index}. Extending end date by 1 day.\")\n",
    "#             end_date = start_date + datetime.timedelta(days=1)\n",
    "\n",
    "#         # Convert dates to string format for GEE\n",
    "#         start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "#         end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "#         # Define AOI (point with buffer)\n",
    "#         poi = ee.Geometry.Point([lon, lat])\n",
    "#         aoi = poi.buffer(buffer_size)\n",
    "\n",
    "#         # Load Sentinel-2 Harmonized data with cloud masking\n",
    "#         s2_collection = (ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
    "#                          .filterBounds(aoi)\n",
    "#                          .filterDate(start_date_str, end_date_str)\n",
    "#                          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))  # Stricter cloud filtering\n",
    "#                          .map(mask_s2_clouds)  # Apply cloud mask\n",
    "#                          .sort(\"CLOUDY_PIXEL_PERCENTAGE\"))\n",
    "\n",
    "#         # Check if images are available\n",
    "#         count = s2_collection.size().getInfo()\n",
    "#         print(f\"Found {count} images for location ({lat}, {lon}) from {start_date_str} to {end_date_str}\")\n",
    "\n",
    "#         if count == 0:\n",
    "#             print(f\"No suitable images found for location ({lat}, {lon}). Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Select the first image from the collection\n",
    "#         image = s2_collection.median().clip(aoi)\n",
    "\n",
    "#         # Visualization Parameters\n",
    "#         rgb_vis = {\n",
    "#             'min': 0.0,\n",
    "#             'max': 0.3,\n",
    "#             'bands': ['B4', 'B3', 'B2'],  # True Color Composite\n",
    "#         }\n",
    "\n",
    "#         # Define export task\n",
    "#         task = ee.batch.Export.image.toDrive(\n",
    "#     image=image,\n",
    "#     description=f\"Sentinel2_Flood_{index}\",\n",
    "#     folder=\"GEE_Flood_Images\",\n",
    "#     fileNamePrefix=f\"Sentinel2_Flood_{index}_{lat}_{lon}\",\n",
    "#     scale=10,\n",
    "#     region=aoi,\n",
    "#     fileFormat='GeoTIFF',\n",
    "#     maxPixels=1e9  # Increase max allowed pixels\n",
    "# )\n",
    "\n",
    "\n",
    "#         # Start the export task\n",
    "#         task.start()\n",
    "#         print(f\"Exporting {index} to Google Drive...\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing location ({lat}, {lon}): {str(e)}\")\n",
    "\n",
    "# print(\"All export tasks initiated! Check Google Drive for results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Flood'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Extract metadata from filename\u001b[39;00m\n\u001b[0;32m     38\u001b[0m metadata \u001b[38;5;241m=\u001b[39m tiff\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m lat, lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(metadata[\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Store extracted features\u001b[39;00m\n\u001b[0;32m     42\u001b[0m satellite_features\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: lat,\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: lon,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDBI\u001b[39m\u001b[38;5;124m\"\u001b[39m: ndbi\n\u001b[0;32m     48\u001b[0m })\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Flood'"
     ]
    }
   ],
   "source": [
    "#  os\n",
    "# import re  # Import regex module\n",
    "\n",
    "# # Initialize a list to store extracted data\n",
    "# satellite_features = []\n",
    "\n",
    "# for tiff in geo_tiffs:\n",
    "#     with rasterio.open(tiff) as src:\n",
    "#         image_array = src.read()  # Read all bands\n",
    "\n",
    "#         # Sentinel-2 Band Mapping\n",
    "#         B4 = image_array[3]  # Red\n",
    "#         B3 = image_array[2]  # Green\n",
    "#         B8 = image_array[7]  # NIR\n",
    "#         B11 = image_array[10] # SWIRimport\n",
    "\n",
    "#         # Compute indices\n",
    "#         ndvi = np.nanmean(calculate_ndvi(B8, B4))\n",
    "#         ndwi = np.nanmean(calculate_ndwi(B3, B8))\n",
    "#         ndbi = np.nanmean(calculate_ndbi(B11, B8))\n",
    "\n",
    "#         # Extract metadata from filename\n",
    "#         filename = os.path.basename(tiff)  # Get filename only\n",
    "#         metadata = filename.replace(\".tif\", \"\").split(\"_\")  # Remove extension and split\n",
    "\n",
    "#         try:\n",
    "#             # Extract latitude from metadata[5]\n",
    "#             lat = float(metadata[5])\n",
    "\n",
    "#             # Extract longitude: Remove the extra `-000000XXXX` part using regex\n",
    "#             lon = float(re.split(r'-', metadata[6])[0])  # Take only the first number\n",
    "\n",
    "#         except ValueError:\n",
    "#             print(f\"Skipping file {filename} due to incorrect metadata format.\")\n",
    "#             continue\n",
    "\n",
    "#         # Store extracted features\n",
    "#         satellite_features.append({\n",
    "#             \"Latitude\": lat,\n",
    "#             \"Longitude\": lon,\n",
    "#             \"NDVI\": ndvi,\n",
    "#             \"NDWI\": ndwi,\n",
    "#             \"NDBI\": ndbi\n",
    "#         })\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_satellite = pd.DataFrame(satellite_features)\n",
    "\n",
    "# # Save extracted features\n",
    "# df_satellite.to_csv(\"new_extracted_satellite_features.csv\", index=False)\n",
    "\n",
    "# print(\"Satellite features extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./datasets/new_satellite_imagery\\\\Sentinel2_Flood_12_28.1657_79.076-0000000000-0000000000.tif', './datasets/new_satellite_imagery\\\\Sentinel2_Flood_12_28.1657_79.076-0000000000-0000007680.tif', './datasets/new_satellite_imagery\\\\Sentinel2_Flood_12_28.1657_79.076-0000007680-0000000000.tif', './datasets/new_satellite_imagery\\\\Sentinel2_Flood_12_28.1657_79.076-0000007680-0000007680.tif', './datasets/new_satellite_imagery\\\\Sentinel2_Flood_13_10.489_79.0039-0000000000-0000000000.tif']\n",
      "['./datasets/new', 'satellite', 'imagery\\\\Sentinel2', 'Flood', '12', '28.1657', '79.076-0000000000-0000000000.tif']\n",
      "['./datasets/new', 'satellite', 'imagery\\\\Sentinel2', 'Flood', '12', '28.1657', '79.076-0000000000-0000007680.tif']\n",
      "['./datasets/new', 'satellite', 'imagery\\\\Sentinel2', 'Flood', '12', '28.1657', '79.076-0000007680-0000000000.tif']\n",
      "['./datasets/new', 'satellite', 'imagery\\\\Sentinel2', 'Flood', '12', '28.1657', '79.076-0000007680-0000007680.tif']\n",
      "['./datasets/new', 'satellite', 'imagery\\\\Sentinel2', 'Flood', '13', '10.489', '79.0039-0000000000-0000000000.tif']\n"
     ]
    }
   ],
   "source": [
    "# # Print the filenames to check the structure\n",
    "# print(geo_tiffs[:5])  # Print first 5 filenames\n",
    "\n",
    "# for tiff in geo_tiffs[:5]:  # Print for first 5 files\n",
    "#     print(tiff.split(\"_\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_27052\\2566850716.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filter.dropna(subset=['Latitude', 'Longitude'], inplace=True)  # Drop rows with NaN coordinates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude  Start Date    End Date   Latitude  Longitude  \\\n",
      "0   33.1767    76.4058  2015-03-20  2015-03-31  33.171409  76.447781   \n",
      "1   10.7121    79.1771  2015-05-17  2015-05-21  10.433041  78.962892   \n",
      "2   26.9195    93.8747  2015-02-06  2015-06-29  26.835759  93.760925   \n",
      "3   21.8734    70.7643  2015-06-24  2015-06-29  21.662200  70.595955   \n",
      "4   26.2074    82.6165  2015-07-15  2015-08-19  26.177276  82.582468   \n",
      "\n",
      "   Rainfall (mm)  Temperature (°C)  Humidity (%)  River Discharge (m³/s)  \\\n",
      "0     131.044676         43.204075     83.279018             1114.604526   \n",
      "1      22.025647         21.785839     84.696733             2718.366764   \n",
      "2     173.153159         38.875367     97.630651             2795.331123   \n",
      "3      47.477868         31.774465     64.771080             1504.834080   \n",
      "4     217.957042         26.952350     28.458914             1788.846557   \n",
      "\n",
      "   Water Level (m)  Elevation (m)  Land Cover Soil Type  Population Density  \\\n",
      "0         2.163148    3870.948571       Urban     Sandy         8173.064808   \n",
      "1         0.425526    5200.065725      Forest      Silt         5307.691399   \n",
      "2         7.153093    4340.979099  Water Body      Loam         2201.165766   \n",
      "3         1.413081    7977.882655      Desert      Silt         5274.123180   \n",
      "4         5.953798    1864.035139  Water Body      Silt         6402.922240   \n",
      "\n",
      "   Infrastructure  Historical Floods  Flood Occurred  \n",
      "0               0                  0               1  \n",
      "1               0                  0               1  \n",
      "2               0                  0               1  \n",
      "3               1                  1               1  \n",
      "4               1                  1               0  \n",
      "Found 49 approximate matches.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ee\n",
    "import datetime\n",
    "\n",
    "# Authenticate & Initialize Earth Engine\n",
    "# ee.Authenticate()\n",
    "# ee.Initialize()\n",
    "\n",
    "# Load historical flood data\n",
    "df_historical = pd.read_csv('./datasets/floods_inventory/India_Floods_Inventory.csv')\n",
    "\n",
    "# Filter data to include only floods after 2015\n",
    "# df_historical['Start Date'] = pd.to_datetime(df_historical['Start Date'], errors='coerce')\n",
    "# df_historical['End Date'] = pd.to_datetime(df_historical['End Date'], errors='coerce')\n",
    "# df_historical = df_historical[df_historical['Start Date'].dt.year >= 2015]\n",
    "df_filter = df_historical[df_historical['Start Date'].str.contains('2015|2016|2017|2018|2019|2020|2021|2022|2023')]\n",
    "df_filter.dropna(subset=['Latitude', 'Longitude'], inplace=True)  # Drop rows with NaN coordinates\n",
    "df_filter.reset_index(drop=True, inplace=True)  # Reset index after dropping rows\n",
    "df_historical=df_filter[['Latitude', 'Longitude', 'Start Date', 'End Date']]\n",
    "df_historical.to_csv('./datasets/floods_inventory/info.csv', index=False)  # Save filtered data\n",
    "\n",
    "df_factors = pd.read_csv('./datasets/india_flood_risk/flood_risk_dataset_india.csv')\n",
    "#make a csv of overlapping data from df_historical and df_factors based on coordinates\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Load your datasets\n",
    "df_hist = pd.read_csv('./datasets/floods_inventory/info.csv')\n",
    "df_fact = pd.read_csv('./datasets/india_flood_risk/flood_risk_dataset_india.csv')\n",
    "\n",
    "# Extract coordinates\n",
    "coords_hist = df_hist[['Latitude', 'Longitude']].to_numpy()\n",
    "coords_fact = df_fact[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# Build KDTree on df_factors\n",
    "tree = cKDTree(coords_fact)\n",
    "\n",
    "# Define a distance threshold (in degrees)\n",
    "# ~0.01 degrees ≈ 1.1 km (very rough approximation)\n",
    "threshold = 0.5\n",
    "\n",
    "# Query tree for all points in df_hist within threshold\n",
    "distances, indices = tree.query(coords_hist, distance_upper_bound=threshold)\n",
    "\n",
    "# Filter matches where distance is within threshold\n",
    "valid = distances != np.inf\n",
    "matched_hist = df_hist[valid].reset_index(drop=True)\n",
    "matched_fact = df_fact.iloc[indices[valid]].reset_index(drop=True)\n",
    "\n",
    "# Combine matched points (optional)\n",
    "matched = pd.concat([matched_hist, matched_fact], axis=1)\n",
    "\n",
    "print(matched.head())\n",
    "print(f\"Found {len(matched)} approximate matches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude  Start Date    End Date   Latitude  Longitude  \\\n",
      "0   33.1767    76.4058  2015-03-20  2015-03-31  33.171409  76.447781   \n",
      "1   10.7121    79.1771  2015-05-17  2015-05-21  10.433041  78.962892   \n",
      "2   26.9195    93.8747  2015-02-06  2015-06-29  26.835759  93.760925   \n",
      "3   21.8734    70.7643  2015-06-24  2015-06-29  21.662200  70.595955   \n",
      "4   26.2074    82.6165  2015-07-15  2015-08-19  26.177276  82.582468   \n",
      "\n",
      "   Rainfall (mm)  Temperature (°C)  Humidity (%)  River Discharge (m³/s)  \\\n",
      "0     131.044676         43.204075     83.279018             1114.604526   \n",
      "1      22.025647         21.785839     84.696733             2718.366764   \n",
      "2     173.153159         38.875367     97.630651             2795.331123   \n",
      "3      47.477868         31.774465     64.771080             1504.834080   \n",
      "4     217.957042         26.952350     28.458914             1788.846557   \n",
      "\n",
      "   Water Level (m)  Elevation (m)  Land Cover Soil Type  Population Density  \\\n",
      "0         2.163148    3870.948571       Urban     Sandy         8173.064808   \n",
      "1         0.425526    5200.065725      Forest      Silt         5307.691399   \n",
      "2         7.153093    4340.979099  Water Body      Loam         2201.165766   \n",
      "3         1.413081    7977.882655      Desert      Silt         5274.123180   \n",
      "4         5.953798    1864.035139  Water Body      Silt         6402.922240   \n",
      "\n",
      "   Infrastructure  Historical Floods  Flood Occurred  \n",
      "0               0                  0               1  \n",
      "1               0                  0               1  \n",
      "2               0                  0               1  \n",
      "3               1                  1               1  \n",
      "4               1                  1               0  \n",
      "Found 49 approximate matches.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Load your datasets\n",
    "df_hist = pd.read_csv('./datasets/floods_inventory/info.csv')\n",
    "df_fact = pd.read_csv('./datasets/india_flood_risk/flood_risk_dataset_india.csv')\n",
    "\n",
    "# Extract coordinates\n",
    "coords_hist = df_hist[['Latitude', 'Longitude']].to_numpy()\n",
    "coords_fact = df_fact[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "# Build KDTree on df_factors\n",
    "tree = cKDTree(coords_fact)\n",
    "\n",
    "# Define a distance threshold (in degrees)\n",
    "# ~0.01 degrees ≈ 1.1 km (very rough approximation)\n",
    "threshold = 0.5\n",
    "\n",
    "# Query tree for all points in df_hist within threshold\n",
    "distances, indices = tree.query(coords_hist, distance_upper_bound=threshold)\n",
    "\n",
    "# Filter matches where distance is within threshold\n",
    "valid = distances != np.inf\n",
    "matched_hist = df_hist[valid].reset_index(drop=True)\n",
    "matched_fact = df_fact.iloc[indices[valid]].reset_index(drop=True)\n",
    "\n",
    "# Combine matched points (optional)\n",
    "matched = pd.concat([matched_hist, matched_fact], axis=1)\n",
    "\n",
    "print(matched.head())\n",
    "print(f\"Found {len(matched)} approximate matches.\")\n",
    "#save to csv\n",
    "matched.to_csv('./datasets/overlapping_data.csv', index=False)  # Save filtered data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images for location (26.2074, 82.6165) from 2015-07-08 to 2015-07-15\n",
      "No suitable images found for location (26.2074, 82.6165). Skipping...\n",
      "Found 0 images for location (26.8946, 93.751) from 2015-08-06 to 2015-08-13\n",
      "No suitable images found for location (26.8946, 93.751). Skipping...\n",
      "Found 0 images for location (11.8278, 78.8554) from 2015-10-04 to 2015-10-11\n",
      "No suitable images found for location (11.8278, 78.8554). Skipping...\n",
      "Found 0 images for location (27.464, 95.6068) from 2016-04-13 to 2016-04-20\n",
      "No suitable images found for location (27.464, 95.6068). Skipping...\n",
      "Found 0 images for location (27.068, 93.949) from 2016-06-22 to 2016-06-29\n",
      "No suitable images found for location (27.068, 93.949). Skipping...\n",
      "Found 0 images for location (22.8107, 80.8349) from 2016-06-30 to 2016-07-07\n",
      "No suitable images found for location (22.8107, 80.8349). Skipping...\n",
      "Found 4 images for location (21.7477, 73.4119) from 2016-01-01 to 2016-01-08\n",
      "Exporting pre-flood imagery 6 to Google Drive...\n",
      "Found 0 images for location (25.6099, 85.1898) from 2016-07-08 to 2016-07-15\n",
      "No suitable images found for location (25.6099, 85.1898). Skipping...\n",
      "Found 0 images for location (17.6536, 80.3401) from 2016-09-14 to 2016-09-21\n",
      "No suitable images found for location (17.6536, 80.3401). Skipping...\n",
      "Found 1 images for location (27.6131, 94.9387) from 2017-01-30 to 2017-02-06\n",
      "Exporting pre-flood imagery 9 to Google Drive...\n",
      "Found 0 images for location (21.898, 70.9375) from 2017-07-08 to 2017-07-15\n",
      "No suitable images found for location (21.898, 70.9375). Skipping...\n",
      "Found 0 images for location (22.9061, 88.0848) from 2017-07-20 to 2017-07-27\n",
      "No suitable images found for location (22.9061, 88.0848). Skipping...\n",
      "Found 0 images for location (28.1657, 79.076) from 2017-10-01 to 2017-10-08\n",
      "No suitable images found for location (28.1657, 79.076). Skipping...\n",
      "Found 0 images for location (10.489, 79.0039) from 2017-10-23 to 2017-10-30\n",
      "No suitable images found for location (10.489, 79.0039). Skipping...\n",
      "Found 0 images for location (14.3584, 75.4408) from 2018-05-22 to 2018-05-29\n",
      "No suitable images found for location (14.3584, 75.4408). Skipping...\n",
      "Found 7 images for location (26.8948, 93.3551) from 2018-11-29 to 2018-12-06\n",
      "Exporting pre-flood imagery 15 to Google Drive...\n",
      "Found 0 images for location (33.3752, 75.1439) from 2018-06-18 to 2018-06-25\n",
      "No suitable images found for location (33.3752, 75.1439). Skipping...\n",
      "Found 1 images for location (20.9795, 73.783) from 2018-06-15 to 2018-06-22\n",
      "Exporting pre-flood imagery 17 to Google Drive...\n",
      "Found 1 images for location (19.5656, 74.5501) from 2018-05-31 to 2018-06-07\n",
      "Exporting pre-flood imagery 18 to Google Drive...\n",
      "Found 2 images for location (12.5238, 75.6883) from 2018-08-31 to 2018-09-07\n",
      "Exporting pre-flood imagery 19 to Google Drive...\n",
      "Found 0 images for location (9.39943, 76.7275) from 2018-07-01 to 2018-07-08\n",
      "No suitable images found for location (9.39943, 76.7275). Skipping...\n",
      "Found 0 images for location (27.2914, 94.1222) from 2018-02-01 to 2018-02-08\n",
      "No suitable images found for location (27.2914, 94.1222). Skipping...\n",
      "Found 0 images for location (25.7354, 82.4309) from 2018-01-02 to 2018-01-09\n",
      "No suitable images found for location (25.7354, 82.4309). Skipping...\n",
      "Found 2 images for location (26.0513, 94.7655) from 2018-08-08 to 2018-08-15\n",
      "Exporting pre-flood imagery 23 to Google Drive...\n",
      "Found 9 images for location (31.3414, 76.6285) from 2018-09-16 to 2018-09-23\n",
      "Exporting pre-flood imagery 24 to Google Drive...\n",
      "Found 7 images for location (17.9, 83.3588) from 2018-11-03 to 2018-11-10\n",
      "Exporting pre-flood imagery 25 to Google Drive...\n",
      "Found 4 images for location (21.5427, 87.4662) from 2019-02-26 to 2019-03-05\n",
      "Exporting pre-flood imagery 26 to Google Drive...\n",
      "Found 2 images for location (23.4991, 91.5736) from 2019-05-17 to 2019-05-24\n",
      "Exporting pre-flood imagery 27 to Google Drive...\n",
      "Found 0 images for location (27.3531, 94.3944) from 2019-06-20 to 2019-06-27\n",
      "No suitable images found for location (27.3531, 94.3944). Skipping...\n",
      "Found 8 images for location (20.2969, 74.748) from 2018-12-31 to 2019-01-07\n",
      "Exporting pre-flood imagery 29 to Google Drive...\n",
      "Found 0 images for location (11.4575, 76.0842) from 2019-06-01 to 2019-06-08\n",
      "No suitable images found for location (11.4575, 76.0842). Skipping...\n",
      "Found 3 images for location (19.1145, 83.4825) from 2019-05-01 to 2019-05-08\n",
      "Exporting pre-flood imagery 31 to Google Drive...\n",
      "Found 0 images for location (25.3928, 74.3521) from 2019-09-06 to 2019-09-13\n",
      "No suitable images found for location (25.3928, 74.3521). Skipping...\n",
      "Found 0 images for location (24.1511, 77.3956) from 2019-09-08 to 2019-09-15\n",
      "No suitable images found for location (24.1511, 77.3956). Skipping...\n",
      "Found 0 images for location (27.0678, 94.2459) from 2019-10-18 to 2019-10-25\n",
      "No suitable images found for location (27.0678, 94.2459). Skipping...\n",
      "Found 0 images for location (10.8488, 78.8554) from 2019-11-23 to 2019-11-30\n",
      "No suitable images found for location (10.8488, 78.8554). Skipping...\n",
      "Found 0 images for location (26.2074, 82.62) from 2015-07-08 to 2015-07-15\n",
      "No suitable images found for location (26.2074, 82.62). Skipping...\n",
      "Found 0 images for location (26.8946, 93.75) from 2015-08-06 to 2015-08-13\n",
      "No suitable images found for location (26.8946, 93.75). Skipping...\n",
      "Invalid coordinates: (118.278, 788.55) at index 38. Skipping...\n",
      "Found 0 images for location (27.464, 95.607) from 2016-04-15 to 2016-04-22\n",
      "No suitable images found for location (27.464, 95.607). Skipping...\n",
      "Found 0 images for location (27.068, 93.95) from 2016-06-18 to 2016-06-25\n",
      "No suitable images found for location (27.068, 93.95). Skipping...\n",
      "Found 0 images for location (17.654, 80.34) from 2016-09-14 to 2016-09-21\n",
      "No suitable images found for location (17.654, 80.34). Skipping...\n",
      "Found 0 images for location (10.489, 79.004) from 2017-10-23 to 2017-10-30\n",
      "No suitable images found for location (10.489, 79.004). Skipping...\n",
      "All export tasks initiated! Check Google Drive for results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ee\n",
    "import datetime\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Load historical flood data\n",
    "df_historical = pd.read_csv('./datasets/floods_inventory/info_dont_use.csv')\n",
    "df_historical.dropna(subset=['Latitude', 'Longitude'], inplace=True)  # Drop rows with NaN coordinates\n",
    "df_historical.reset_index(drop=True, inplace=True)  # Reset index after dropping rows\n",
    "\n",
    "# Filter data to include only floods after 2015\n",
    "df_historical['Start Date'] = pd.to_datetime(df_historical['Start Date'], errors='coerce')\n",
    "df_historical['End Date'] = pd.to_datetime(df_historical['End Date'], errors='coerce')\n",
    "df_historical = df_historical[df_historical['Start Date'].dt.year >= 2015]\n",
    "\n",
    "# Define buffer size (e.g., 50 km)\n",
    "buffer_size = 50000  # 50 km\n",
    "\n",
    "# Cloud Mask Function (QA60-based)\n",
    "def mask_s2_clouds(image):\n",
    "    qa = image.select(\"QA60\")  # Sentinel-2 cloud mask band\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "\n",
    "    # Both flags should be zero for clear pixels\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "        qa.bitwiseAnd(cirrus_bit_mask).eq(0)\n",
    "    )\n",
    "\n",
    "    # Apply mask and scale the image\n",
    "    return image.updateMask(mask).divide(10000)\n",
    "\n",
    "# Loop through each row and extract imagery\n",
    "for index, row in df_historical.iterrows():\n",
    "    try:\n",
    "        lat, lon = row['Latitude'], row['Longitude']\n",
    "        \n",
    "        # Validate coordinates\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            print(f\"Invalid coordinates: ({lat}, {lon}) at index {index}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        start_date = row['Start Date']\n",
    "        \n",
    "        # Calculate t-7 date (7 days before start date)\n",
    "        t_minus_7_date = start_date - datetime.timedelta(days=7)\n",
    "\n",
    "        # Convert dates to string format for GEE\n",
    "        t_minus_7_date_str = t_minus_7_date.strftime('%Y-%m-%d')\n",
    "        start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Define AOI (point with buffer)\n",
    "        poi = ee.Geometry.Point([lon, lat])\n",
    "        aoi = poi.buffer(buffer_size)\n",
    "\n",
    "        # Load Sentinel-2 Harmonized data with cloud masking\n",
    "        s2_collection = (ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
    "                         .filterBounds(aoi)\n",
    "                         .filterDate(t_minus_7_date_str, start_date_str)\n",
    "                         .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))  # Stricter cloud filtering\n",
    "                         .map(mask_s2_clouds)  # Apply cloud mask\n",
    "                         .sort(\"CLOUDY_PIXEL_PERCENTAGE\"))\n",
    "\n",
    "        # Check if images are available\n",
    "        count = s2_collection.size().getInfo()\n",
    "        print(f\"Found {count} images for location ({lat}, {lon}) from {t_minus_7_date_str} to {start_date_str}\")\n",
    "\n",
    "        if count == 0:\n",
    "            print(f\"No suitable images found for location ({lat}, {lon}). Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Select the median image from the collection\n",
    "        image = s2_collection.median().clip(aoi)\n",
    "\n",
    "        # Visualization Parameters\n",
    "        rgb_vis = {\n",
    "            'min': 0.0,\n",
    "            'max': 0.3,\n",
    "            'bands': ['B4', 'B3', 'B2'],  # True Color Composite\n",
    "        }\n",
    "\n",
    "        # Define export task\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image,\n",
    "            description=f\"PreFlood_t-7_to_t-0_{index}\",\n",
    "            folder=\"GEE_PreFlood_Images\",\n",
    "            fileNamePrefix=f\"PreFlood_t-7_to_t-0_{index}_{lat}_{lon}\",\n",
    "            scale=10,\n",
    "            region=aoi,\n",
    "            fileFormat='GeoTIFF',\n",
    "            maxPixels=1e9  # Increase max allowed pixels\n",
    "        )\n",
    "\n",
    "        # Start the export task\n",
    "        task.start()\n",
    "        print(f\"Exporting pre-flood imagery {index} to Google Drive...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing location ({lat}, {lon}): {str(e)}\")\n",
    "\n",
    "print(\"All export tasks initiated! Check Google Drive for results.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=WBaIj51kGmkwcT8YdFLXx0arCsudQphJaZJMWsRnr8M&tc=pVZ9DRQkfR-Hx1EZoRZZnizfWZVE9MdJyjCA9SM5ja0&cc=xXoixdyv5lvlO127MgCCCkrqWTpUKLxMkn5rc1YF4og>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=WBaIj51kGmkwcT8YdFLXx0arCsudQphJaZJMWsRnr8M&tc=pVZ9DRQkfR-Hx1EZoRZZnizfWZVE9MdJyjCA9SM5ja0&cc=xXoixdyv5lvlO127MgCCCkrqWTpUKLxMkn5rc1YF4og</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Found 2 non-flood images from 2024-03-16 to 2024-03-23 at (26.2074, 82.6165)\n",
      "[0] Exporting non-flood image to Drive...\n",
      "[1] Found 4 non-flood images from 2023-11-16 to 2023-11-23 at (26.8946, 93.751)\n",
      "[1] Exporting non-flood image to Drive...\n",
      "[2] Found 4 non-flood images from 2024-09-12 to 2024-09-19 at (11.8278, 78.8554)\n",
      "[2] Exporting non-flood image to Drive...\n",
      "[3] Found 2 non-flood images from 2023-07-15 to 2023-07-22 at (27.464, 95.6068)\n",
      "[3] Exporting non-flood image to Drive...\n",
      "[4] Found 0 non-flood images from 2022-05-19 to 2022-05-26 at (27.068, 93.949)\n",
      "[4] No non-flood images available. Skipping.\n",
      "[5] Found 7 non-flood images from 2023-02-18 to 2023-02-25 at (22.8107, 80.8349)\n",
      "[5] Exporting non-flood image to Drive...\n",
      "[6] Found 0 non-flood images from 2022-08-11 to 2022-08-18 at (21.7477, 73.4119)\n",
      "[6] No non-flood images available. Skipping.\n",
      "[7] Found 6 non-flood images from 2023-01-11 to 2023-01-18 at (25.6099, 85.1898)\n",
      "[7] Exporting non-flood image to Drive...\n",
      "[8] Found 0 non-flood images from 2024-01-20 to 2024-01-27 at (17.6536, 80.3401)\n",
      "[8] No non-flood images available. Skipping.\n",
      "[9] Found 0 non-flood images from 2024-05-05 to 2024-05-12 at (27.6131, 94.9387)\n",
      "[9] No non-flood images available. Skipping.\n",
      "[10] Found 7 non-flood images from 2022-04-12 to 2022-04-19 at (21.898, 70.9375)\n",
      "[10] Exporting non-flood image to Drive...\n",
      "[11] Found 0 non-flood images from 2022-08-04 to 2022-08-11 at (22.9061, 88.0848)\n",
      "[11] No non-flood images available. Skipping.\n",
      "[12] Found 2 non-flood images from 2022-11-05 to 2022-11-12 at (28.1657, 79.076)\n",
      "[12] Exporting non-flood image to Drive...\n",
      "[13] Found 5 non-flood images from 2022-01-21 to 2022-01-28 at (10.489, 79.0039)\n",
      "[13] Exporting non-flood image to Drive...\n",
      "[14] Found 0 non-flood images from 2022-06-02 to 2022-06-09 at (14.3584, 75.4408)\n",
      "[14] No non-flood images available. Skipping.\n",
      "[15] Found 0 non-flood images from 2024-09-12 to 2024-09-19 at (26.8948, 93.3551)\n",
      "[15] No non-flood images available. Skipping.\n",
      "[16] Found 6 non-flood images from 2022-02-17 to 2022-02-24 at (33.3752, 75.1439)\n",
      "[16] Exporting non-flood image to Drive...\n",
      "[17] Found 6 non-flood images from 2023-03-06 to 2023-03-13 at (20.9795, 73.783)\n",
      "[17] Exporting non-flood image to Drive...\n",
      "[18] Found 0 non-flood images from 2022-05-05 to 2022-05-12 at (19.5656, 74.5501)\n",
      "[18] No non-flood images available. Skipping.\n",
      "[19] Found 1 non-flood images from 2024-02-13 to 2024-02-20 at (12.5238, 75.6883)\n",
      "[19] Exporting non-flood image to Drive...\n",
      "[20] Found 0 non-flood images from 2022-05-11 to 2022-05-18 at (9.39943, 76.7275)\n",
      "[20] No non-flood images available. Skipping.\n",
      "[21] Found 1 non-flood images from 2023-01-11 to 2023-01-18 at (27.2914, 94.1222)\n",
      "[21] Exporting non-flood image to Drive...\n",
      "[22] Found 7 non-flood images from 2024-02-11 to 2024-02-18 at (25.7354, 82.4309)\n",
      "[22] Exporting non-flood image to Drive...\n",
      "[23] Found 6 non-flood images from 2024-12-20 to 2024-12-27 at (26.0513, 94.7655)\n",
      "[23] Exporting non-flood image to Drive...\n",
      "[24] Found 3 non-flood images from 2024-01-07 to 2024-01-14 at (31.3414, 76.6285)\n",
      "[24] Exporting non-flood image to Drive...\n",
      "[25] Found 6 non-flood images from 2023-12-13 to 2023-12-20 at (17.9, 83.3588)\n",
      "[25] Exporting non-flood image to Drive...\n",
      "[26] Found 0 non-flood images from 2023-08-07 to 2023-08-14 at (21.5427, 87.4662)\n",
      "[26] No non-flood images available. Skipping.\n",
      "[27] Found 7 non-flood images from 2022-01-08 to 2022-01-15 at (23.4991, 91.5736)\n",
      "[27] Exporting non-flood image to Drive...\n",
      "[28] Found 0 non-flood images from 2024-05-05 to 2024-05-12 at (27.3531, 94.3944)\n",
      "[28] No non-flood images available. Skipping.\n",
      "[29] Found 4 non-flood images from 2023-02-01 to 2023-02-08 at (20.2969, 74.748)\n",
      "[29] Exporting non-flood image to Drive...\n",
      "[30] Found 3 non-flood images from 2023-04-14 to 2023-04-21 at (11.4575, 76.0842)\n",
      "[30] Exporting non-flood image to Drive...\n",
      "[31] Found 0 non-flood images from 2023-08-01 to 2023-08-08 at (19.1145, 83.4825)\n",
      "[31] No non-flood images available. Skipping.\n",
      "[32] Found 3 non-flood images from 2023-01-13 to 2023-01-20 at (25.3928, 74.3521)\n",
      "[32] Exporting non-flood image to Drive...\n",
      "[33] Found 0 non-flood images from 2024-09-07 to 2024-09-14 at (24.1511, 77.3956)\n",
      "[33] No non-flood images available. Skipping.\n",
      "[34] Found 3 non-flood images from 2022-02-20 to 2022-02-27 at (27.0678, 94.2459)\n",
      "[34] Exporting non-flood image to Drive...\n",
      "[35] Found 0 non-flood images from 2022-06-17 to 2022-06-24 at (10.8488, 78.8554)\n",
      "[35] No non-flood images available. Skipping.\n",
      "[36] Found 6 non-flood images from 2023-10-18 to 2023-10-25 at (26.2074, 82.62)\n",
      "[36] Exporting non-flood image to Drive...\n",
      "[37] Found 2 non-flood images from 2023-03-07 to 2023-03-14 at (26.8946, 93.75)\n",
      "[37] Exporting non-flood image to Drive...\n",
      "Invalid coordinates: (118.278, 788.55) at index 38. Skipping...\n",
      "[39] Found 3 non-flood images from 2022-12-14 to 2022-12-21 at (27.464, 95.607)\n",
      "[39] Exporting non-flood image to Drive...\n",
      "[40] Found 0 non-flood images from 2022-09-21 to 2022-09-28 at (27.068, 93.95)\n",
      "[40] No non-flood images available. Skipping.\n",
      "[41] Found 3 non-flood images from 2024-06-11 to 2024-06-18 at (17.654, 80.34)\n",
      "[41] Exporting non-flood image to Drive...\n",
      "[42] Found 0 non-flood images from 2024-04-11 to 2024-04-18 at (10.489, 79.004)\n",
      "[42] No non-flood images available. Skipping.\n",
      "✅ All non-flood export tasks initiated! Check Google Drive for results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ee\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "# Load flood inventory data\n",
    "df_historical = pd.read_csv('./datasets/floods_inventory/info_dont_use.csv')\n",
    "df_historical.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
    "df_historical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Cloud Mask Function\n",
    "def mask_s2_clouds(image):\n",
    "    qa = image.select(\"QA60\")\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    return image.updateMask(mask).divide(10000)\n",
    "\n",
    "# Constants\n",
    "buffer_size = 50000  # 50 km buffer\n",
    "start_year = 2022\n",
    "end_year = 2024\n",
    "export_folder = \"GEE_NonFlood_Images\"\n",
    "\n",
    "# Loop through each coordinate and pick a random week after 2022\n",
    "for index, row in df_historical.iterrows():\n",
    "    try:\n",
    "        lat, lon = row['Latitude'], row['Longitude']\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            print(f\"Invalid coordinates: ({lat}, {lon}) at index {index}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Generate random date after 2022\n",
    "        random_year = random.randint(start_year, end_year)\n",
    "        random_month = random.randint(1, 12)\n",
    "        random_day = random.randint(1, 21)  # So that t+7 doesn't exceed month end\n",
    "\n",
    "        random_date = datetime.date(random_year, random_month, random_day)\n",
    "        random_date_plus_7 = random_date + datetime.timedelta(days=7)\n",
    "\n",
    "        start_date_str = random_date.strftime('%Y-%m-%d')\n",
    "        end_date_str = random_date_plus_7.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Area of Interest\n",
    "        poi = ee.Geometry.Point([lon, lat])\n",
    "        aoi = poi.buffer(buffer_size)\n",
    "\n",
    "        # Sentinel-2 data with filters\n",
    "        s2_collection = (ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
    "                         .filterBounds(aoi)\n",
    "                         .filterDate(start_date_str, end_date_str)\n",
    "                         .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "                         .map(mask_s2_clouds)\n",
    "                         .sort(\"CLOUDY_PIXEL_PERCENTAGE\"))\n",
    "\n",
    "        count = s2_collection.size().getInfo()\n",
    "        print(f\"[{index}] Found {count} non-flood images from {start_date_str} to {end_date_str} at ({lat}, {lon})\")\n",
    "\n",
    "        if count == 0:\n",
    "            print(f\"[{index}] No non-flood images available. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Select median image\n",
    "        image = s2_collection.median().clip(aoi)\n",
    "\n",
    "        # Export task\n",
    "        task = ee.batch.Export.image.toDrive(\n",
    "            image=image,\n",
    "            description=f\"NonFlood_{index}\",\n",
    "            folder=export_folder,\n",
    "            fileNamePrefix=f\"NonFlood_{index}_{lat}_{lon}\",\n",
    "            scale=10,\n",
    "            region=aoi,\n",
    "            fileFormat='GeoTIFF',\n",
    "            maxPixels=1e9\n",
    "        )\n",
    "        task.start()\n",
    "        print(f\"[{index}] Exporting non-flood image to Drive...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{index}] Error processing location ({lat}, {lon}): {str(e)}\")\n",
    "\n",
    "print(\"✅ All non-flood export tasks initiated! Check Google Drive for results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Obtaining dependency information for rasterio from https://files.pythonhosted.org/packages/7e/1f/56462740694de764fde264051224fcbf800dad43cac92a66753153128866/rasterio-1.4.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading rasterio-1.4.3-cp311-cp311-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Obtaining dependency information for affine from https://files.pythonhosted.org/packages/0b/f7/85273299ab57117850cc0a936c64151171fac4da49bc6fba0dad984a7c5f/affine-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rasterio) (22.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rasterio) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rasterio) (8.0.4)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Obtaining dependency information for cligj>=0.5 from https://files.pythonhosted.org/packages/73/86/43fa9f15c5b9fb6e82620428827cd3c284aa933431405d1bcf5231ae3d3e/cligj-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rasterio) (1.24.3)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Obtaining dependency information for click-plugins from https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rasterio) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Downloading rasterio-1.4.3-cp311-cp311-win_amd64.whl (25.5 MB)\n",
      "   ---------------------------------------- 0.0/25.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/25.5 MB 8.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.6/25.5 MB 17.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.3/25.5 MB 23.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.3/25.5 MB 23.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.7/25.5 MB 24.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.1/25.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.9/25.5 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.9/25.5 MB 27.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.8/25.5 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.1/25.5 MB 29.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.6/25.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.8/25.5 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.5/25.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.8/25.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.0/25.5 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.0/25.5 MB 29.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.5/25.5 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Installing collected packages: affine, cligj, click-plugins, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rasterio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
